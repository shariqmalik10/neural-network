{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from NNEngine_v3 import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'SeniorCitizen',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'tenure',\n",
       " 'PhoneService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'PaperlessBilling',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'Churn',\n",
       " 'IS_DSL',\n",
       " 'IS_Fiber optic',\n",
       " 'IS_No',\n",
       " 'Contract_Month-to-month',\n",
       " 'Contract_One year',\n",
       " 'Contract_Two year',\n",
       " 'PM_Bank transfer (automatic)',\n",
       " 'PM_Credit card (automatic)',\n",
       " 'PM_Electronic check',\n",
       " 'PM_Mailed check',\n",
       " 'ML_No',\n",
       " 'ML_No phone service',\n",
       " 'ML_Yes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(df[\"Churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(\"Churn\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300 - Total Loss: 5.307250707741886\n",
      "Epoch: 2/300 - Total Loss: 5.36921710922066\n",
      "Epoch: 3/300 - Total Loss: 5.373603961496289\n",
      "Epoch: 4/300 - Total Loss: 5.3723979729404325\n",
      "Epoch: 5/300 - Total Loss: 5.37291985504583\n",
      "Epoch: 6/300 - Total Loss: 5.374281840282866\n",
      "Epoch: 7/300 - Total Loss: 5.374687789645877\n",
      "Epoch: 8/300 - Total Loss: 5.373564444911593\n",
      "Epoch: 9/300 - Total Loss: 5.371308547359748\n",
      "Epoch: 10/300 - Total Loss: 5.368498621699022\n",
      "Epoch: 11/300 - Total Loss: 5.365559531272758\n",
      "Epoch: 12/300 - Total Loss: 5.362735138518344\n",
      "Epoch: 13/300 - Total Loss: 5.360141799460188\n",
      "Epoch: 14/300 - Total Loss: 5.357820317142115\n",
      "Epoch: 15/300 - Total Loss: 5.355771160623968\n",
      "Epoch: 16/300 - Total Loss: 5.3539754837391\n",
      "Epoch: 17/300 - Total Loss: 5.352406799712583\n",
      "Epoch: 18/300 - Total Loss: 5.351037080505806\n",
      "Epoch: 19/300 - Total Loss: 5.349839705958777\n",
      "Epoch: 20/300 - Total Loss: 5.348790705341161\n",
      "Epoch: 21/300 - Total Loss: 5.347869112653608\n",
      "Epoch: 22/300 - Total Loss: 5.3470568890078845\n",
      "Epoch: 23/300 - Total Loss: 5.346338655288907\n",
      "Epoch: 24/300 - Total Loss: 5.345701361246278\n",
      "Epoch: 25/300 - Total Loss: 5.345133953261083\n",
      "Epoch: 26/300 - Total Loss: 5.344627068878983\n",
      "Epoch: 27/300 - Total Loss: 5.344172768441889\n",
      "Epoch: 28/300 - Total Loss: 5.343764305301519\n",
      "Epoch: 29/300 - Total Loss: 5.343395932010789\n",
      "Epoch: 30/300 - Total Loss: 5.343062738287372\n",
      "Epoch: 31/300 - Total Loss: 5.342760516192675\n",
      "Epoch: 32/300 - Total Loss: 5.342485648207014\n",
      "Epoch: 33/300 - Total Loss: 5.3422350143612025\n",
      "Epoch: 34/300 - Total Loss: 5.342005915129713\n",
      "Epoch: 35/300 - Total Loss: 5.341796007316887\n",
      "Epoch: 36/300 - Total Loss: 5.341603250639162\n",
      "Epoch: 37/300 - Total Loss: 5.34142586311224\n",
      "Epoch: 38/300 - Total Loss: 5.3412622836929655\n",
      "Epoch: 39/300 - Total Loss: 5.341111140908062\n",
      "Epoch: 40/300 - Total Loss: 5.34097122643339\n",
      "Epoch: 41/300 - Total Loss: 5.340841472776474\n",
      "Epoch: 42/300 - Total Loss: 5.340720934368683\n",
      "Epoch: 43/300 - Total Loss: 5.340608771498478\n",
      "Epoch: 44/300 - Total Loss: 5.340504236618523\n",
      "Epoch: 45/300 - Total Loss: 5.340406662641969\n",
      "Epoch: 46/300 - Total Loss: 5.340315452910353\n",
      "Epoch: 47/300 - Total Loss: 5.340230072570262\n",
      "Epoch: 48/300 - Total Loss: 5.34015004114065\n",
      "Epoch: 49/300 - Total Loss: 5.340074926089278\n",
      "Epoch: 50/300 - Total Loss: 5.3400043372667945\n",
      "Epoch: 51/300 - Total Loss: 5.33993792207173\n",
      "Epoch: 52/300 - Total Loss: 5.339875361240023\n",
      "Epoch: 53/300 - Total Loss: 5.33981636516963\n",
      "Epoch: 54/300 - Total Loss: 5.339760670704695\n",
      "Epoch: 55/300 - Total Loss: 5.339708038315444\n",
      "Epoch: 56/300 - Total Loss: 5.33965824961966\n",
      "Epoch: 57/300 - Total Loss: 5.33961110519967\n",
      "Epoch: 58/300 - Total Loss: 5.339566422675641\n",
      "Epoch: 59/300 - Total Loss: 5.339524035001614\n",
      "Epoch: 60/300 - Total Loss: 5.339483788955587\n",
      "Epoch: 61/300 - Total Loss: 5.339445543798982\n",
      "Epoch: 62/300 - Total Loss: 5.33940917008429\n",
      "Epoch: 63/300 - Total Loss: 5.339374548592573\n",
      "Epoch: 64/300 - Total Loss: 5.33934156938507\n",
      "Epoch: 65/300 - Total Loss: 5.33931013095515\n",
      "Epoch: 66/300 - Total Loss: 5.3392801394687766\n",
      "Epoch: 67/300 - Total Loss: 5.339251508083125\n",
      "Epoch: 68/300 - Total Loss: 5.33922415633436\n",
      "Epoch: 69/300 - Total Loss: 5.339198009586711\n",
      "Epoch: 70/300 - Total Loss: 5.339172998535949\n",
      "Epoch: 71/300 - Total Loss: 5.339149058761288\n",
      "Epoch: 72/300 - Total Loss: 5.33912613032037\n",
      "Epoch: 73/300 - Total Loss: 5.33910415738271\n",
      "Epoch: 74/300 - Total Loss: 5.339083087897507\n",
      "Epoch: 75/300 - Total Loss: 5.339062873292195\n",
      "Epoch: 76/300 - Total Loss: 5.3390434681985415\n",
      "Epoch: 77/300 - Total Loss: 5.339024830203477\n",
      "Epoch: 78/300 - Total Loss: 5.339006919622128\n",
      "Epoch: 79/300 - Total Loss: 5.338989699290854\n",
      "Epoch: 80/300 - Total Loss: 5.338973134378291\n",
      "Epoch: 81/300 - Total Loss: 5.338957192212636\n",
      "Epoch: 82/300 - Total Loss: 5.3389418421236305\n",
      "Epoch: 83/300 - Total Loss: 5.338927055297795\n",
      "Epoch: 84/300 - Total Loss: 5.338912804645702\n",
      "Epoch: 85/300 - Total Loss: 5.3388990646801435\n",
      "Epoch: 86/300 - Total Loss: 5.338885811404175\n",
      "Epoch: 87/300 - Total Loss: 5.338873022208176\n",
      "Epoch: 88/300 - Total Loss: 5.338860675775045\n",
      "Epoch: 89/300 - Total Loss: 5.338848751992876\n",
      "Epoch: 90/300 - Total Loss: 5.338837231874396\n",
      "Epoch: 91/300 - Total Loss: 5.338826097482602\n",
      "Epoch: 92/300 - Total Loss: 5.338815331862058\n",
      "Epoch: 93/300 - Total Loss: 5.338804918975351\n",
      "Epoch: 94/300 - Total Loss: 5.338794843644285\n",
      "Epoch: 95/300 - Total Loss: 5.338785091495416\n",
      "Epoch: 96/300 - Total Loss: 5.338775648909546\n",
      "Epoch: 97/300 - Total Loss: 5.338766502974883\n",
      "Epoch: 98/300 - Total Loss: 5.338757641443532\n",
      "Epoch: 99/300 - Total Loss: 5.3387490526910835\n",
      "Epoch: 100/300 - Total Loss: 5.3387407256790205\n",
      "Epoch: 101/300 - Total Loss: 5.3387326499197485\n",
      "Epoch: 102/300 - Total Loss: 5.338724815444024\n",
      "Epoch: 103/300 - Total Loss: 5.338717212770593\n",
      "Epoch: 104/300 - Total Loss: 5.338709832877902\n",
      "Epoch: 105/300 - Total Loss: 5.338702667177674\n",
      "Epoch: 106/300 - Total Loss: 5.3386957074902455\n",
      "Epoch: 107/300 - Total Loss: 5.338688946021527\n",
      "Epoch: 108/300 - Total Loss: 5.338682375341454\n",
      "Epoch: 109/300 - Total Loss: 5.338675988363824\n",
      "Epoch: 110/300 - Total Loss: 5.338669778327436\n",
      "Epoch: 111/300 - Total Loss: 5.338663738778396\n",
      "Epoch: 112/300 - Total Loss: 5.3386578635535695\n",
      "Epoch: 113/300 - Total Loss: 5.33865214676503\n",
      "Epoch: 114/300 - Total Loss: 5.33864658278548\n",
      "Epoch: 115/300 - Total Loss: 5.338641166234573\n",
      "Epoch: 116/300 - Total Loss: 5.3386358919660415\n",
      "Epoch: 117/300 - Total Loss: 5.338630755055619\n",
      "Epoch: 118/300 - Total Loss: 5.33862575078967\n",
      "Epoch: 119/300 - Total Loss: 5.338620874654498\n",
      "Epoch: 120/300 - Total Loss: 5.338616122326278\n",
      "Epoch: 121/300 - Total Loss: 5.3386114896615755\n",
      "Epoch: 122/300 - Total Loss: 5.338606972688402\n",
      "Epoch: 123/300 - Total Loss: 5.338602567597803\n",
      "Epoch: 124/300 - Total Loss: 5.338598270735905\n",
      "Epoch: 125/300 - Total Loss: 5.338594078596416\n",
      "Epoch: 126/300 - Total Loss: 5.338589987813547\n",
      "Epoch: 127/300 - Total Loss: 5.338585995155325\n",
      "Epoch: 128/300 - Total Loss: 5.338582097517271\n",
      "Epoch: 129/300 - Total Loss: 5.338578291916424\n",
      "Epoch: 130/300 - Total Loss: 5.338574575485693\n",
      "Epoch: 131/300 - Total Loss: 5.338570945468503\n",
      "Epoch: 132/300 - Total Loss: 5.338567399213739\n",
      "Epoch: 133/300 - Total Loss: 5.338563934170946\n",
      "Epoch: 134/300 - Total Loss: 5.33856054788579\n",
      "Epoch: 135/300 - Total Loss: 5.338557237995751\n",
      "Epoch: 136/300 - Total Loss: 5.338554002226034\n",
      "Epoch: 137/300 - Total Loss: 5.338550838385708\n",
      "Epoch: 138/300 - Total Loss: 5.338547744364024\n",
      "Epoch: 139/300 - Total Loss: 5.338544718126923\n",
      "Epoch: 140/300 - Total Loss: 5.3385417577137275\n",
      "Epoch: 141/300 - Total Loss: 5.338538861233989\n",
      "Epoch: 142/300 - Total Loss: 5.338536026864506\n",
      "Epoch: 143/300 - Total Loss: 5.338533252846467\n",
      "Epoch: 144/300 - Total Loss: 5.338530537482752\n",
      "Epoch: 145/300 - Total Loss: 5.338527879135356\n",
      "Epoch: 146/300 - Total Loss: 5.338525276222942\n",
      "Epoch: 147/300 - Total Loss: 5.338522727218512\n",
      "Epoch: 148/300 - Total Loss: 5.338520230647176\n",
      "Epoch: 149/300 - Total Loss: 5.3385177850840435\n",
      "Epoch: 150/300 - Total Loss: 5.3385153891522075\n",
      "Epoch: 151/300 - Total Loss: 5.338513041520814\n",
      "Epoch: 152/300 - Total Loss: 5.3385107409032395\n",
      "Epoch: 153/300 - Total Loss: 5.338508486055333\n",
      "Epoch: 154/300 - Total Loss: 5.338506275773752\n",
      "Epoch: 155/300 - Total Loss: 5.338504108894368\n",
      "Epoch: 156/300 - Total Loss: 5.338501984290748\n",
      "Epoch: 157/300 - Total Loss: 5.338499900872693\n",
      "Epoch: 158/300 - Total Loss: 5.338497857584863\n",
      "Epoch: 159/300 - Total Loss: 5.338495853405439\n",
      "Epoch: 160/300 - Total Loss: 5.338493887344864\n",
      "Epoch: 161/300 - Total Loss: 5.338491958444626\n",
      "Epoch: 162/300 - Total Loss: 5.338490065776095\n",
      "Epoch: 163/300 - Total Loss: 5.338488208439422\n",
      "Epoch: 164/300 - Total Loss: 5.33848638556247\n",
      "Epoch: 165/300 - Total Loss: 5.338484596299799\n",
      "Epoch: 166/300 - Total Loss: 5.338482839831699\n",
      "Epoch: 167/300 - Total Loss: 5.338481115363245\n",
      "Epoch: 168/300 - Total Loss: 5.3384794221234175\n",
      "Epoch: 169/300 - Total Loss: 5.338477759364239\n",
      "Epoch: 170/300 - Total Loss: 5.338476126359954\n",
      "Epoch: 171/300 - Total Loss: 5.3384745224062415\n",
      "Epoch: 172/300 - Total Loss: 5.3384729468194685\n",
      "Epoch: 173/300 - Total Loss: 5.338471398935959\n",
      "Epoch: 174/300 - Total Loss: 5.338469878111303\n",
      "Epoch: 175/300 - Total Loss: 5.338468383719683\n",
      "Epoch: 176/300 - Total Loss: 5.338466915153247\n",
      "Epoch: 177/300 - Total Loss: 5.338465471821487\n",
      "Epoch: 178/300 - Total Loss: 5.338464053150647\n",
      "Epoch: 179/300 - Total Loss: 5.338462658583162\n",
      "Epoch: 180/300 - Total Loss: 5.338461287577114\n",
      "Epoch: 181/300 - Total Loss: 5.338459939605699\n",
      "Epoch: 182/300 - Total Loss: 5.33845861415674\n",
      "Epoch: 183/300 - Total Loss: 5.338457310732189\n",
      "Epoch: 184/300 - Total Loss: 5.338456028847672\n",
      "Epoch: 185/300 - Total Loss: 5.338454768032034\n",
      "Epoch: 186/300 - Total Loss: 5.338453527826915\n",
      "Epoch: 187/300 - Total Loss: 5.338452307786336\n",
      "Epoch: 188/300 - Total Loss: 5.338451107476292\n",
      "Epoch: 189/300 - Total Loss: 5.338449926474381\n",
      "Epoch: 190/300 - Total Loss: 5.338448764369423\n",
      "Epoch: 191/300 - Total Loss: 5.338447620761114\n",
      "Epoch: 192/300 - Total Loss: 5.338446495259676\n",
      "Epoch: 193/300 - Total Loss: 5.338445387485527\n",
      "Epoch: 194/300 - Total Loss: 5.338444297068973\n",
      "Epoch: 195/300 - Total Loss: 5.338443223649886\n",
      "Epoch: 196/300 - Total Loss: 5.33844216687742\n",
      "Epoch: 197/300 - Total Loss: 5.338441126409721\n",
      "Epoch: 198/300 - Total Loss: 5.338440101913656\n",
      "Epoch: 199/300 - Total Loss: 5.338439093064539\n",
      "Epoch: 200/300 - Total Loss: 5.33843809954589\n",
      "Epoch: 201/300 - Total Loss: 5.33843712104917\n",
      "Epoch: 202/300 - Total Loss: 5.338436157273559\n",
      "Epoch: 203/300 - Total Loss: 5.338435207925719\n",
      "Epoch: 204/300 - Total Loss: 5.338434272719567\n",
      "Epoch: 205/300 - Total Loss: 5.338433351376073\n",
      "Epoch: 206/300 - Total Loss: 5.338432443623044\n",
      "Epoch: 207/300 - Total Loss: 5.338431549194924\n",
      "Epoch: 208/300 - Total Loss: 5.338430667832604\n",
      "Epoch: 209/300 - Total Loss: 5.338429799283234\n",
      "Epoch: 210/300 - Total Loss: 5.338428943300045\n",
      "Epoch: 211/300 - Total Loss: 5.3384280996421705\n",
      "Epoch: 212/300 - Total Loss: 5.338427268074482\n",
      "Epoch: 213/300 - Total Loss: 5.3384264483674215\n",
      "Epoch: 214/300 - Total Loss: 5.338425640296851\n",
      "Epoch: 215/300 - Total Loss: 5.33842484364389\n",
      "Epoch: 216/300 - Total Loss: 5.338424058194776\n",
      "Epoch: 217/300 - Total Loss: 5.338423283740723\n",
      "Epoch: 218/300 - Total Loss: 5.338422520077773\n",
      "Epoch: 219/300 - Total Loss: 5.338421767006678\n",
      "Epoch: 220/300 - Total Loss: 5.338421024332753\n",
      "Epoch: 221/300 - Total Loss: 5.338420291865767\n",
      "Epoch: 222/300 - Total Loss: 5.338419569419809\n",
      "Epoch: 223/300 - Total Loss: 5.338418856813185\n",
      "Epoch: 224/300 - Total Loss: 5.338418153868284\n",
      "Epoch: 225/300 - Total Loss: 5.338417460411487\n",
      "Epoch: 226/300 - Total Loss: 5.3384167762730526\n",
      "Epoch: 227/300 - Total Loss: 5.338416101287012\n",
      "Epoch: 228/300 - Total Loss: 5.33841543529107\n",
      "Epoch: 229/300 - Total Loss: 5.33841477812651\n",
      "Epoch: 230/300 - Total Loss: 5.338414129638099\n",
      "Epoch: 231/300 - Total Loss: 5.338413489673996\n",
      "Epoch: 232/300 - Total Loss: 5.338412858085661\n",
      "Epoch: 233/300 - Total Loss: 5.338412234727772\n",
      "Epoch: 234/300 - Total Loss: 5.338411619458146\n",
      "Epoch: 235/300 - Total Loss: 5.338411012137648\n",
      "Epoch: 236/300 - Total Loss: 5.338410412630123\n",
      "Epoch: 237/300 - Total Loss: 5.33840982080231\n",
      "Epoch: 238/300 - Total Loss: 5.33840923652378\n",
      "Epoch: 239/300 - Total Loss: 5.338408659666853\n",
      "Epoch: 240/300 - Total Loss: 5.338408090106538\n",
      "Epoch: 241/300 - Total Loss: 5.338407527720459\n",
      "Epoch: 242/300 - Total Loss: 5.338406972388792\n",
      "Epoch: 243/300 - Total Loss: 5.338406423994201\n",
      "Epoch: 244/300 - Total Loss: 5.338405882421781\n",
      "Epoch: 245/300 - Total Loss: 5.338405347558987\n",
      "Epoch: 246/300 - Total Loss: 5.338404819295588\n",
      "Epoch: 247/300 - Total Loss: 5.338404297523605\n",
      "Epoch: 248/300 - Total Loss: 5.338403782137249\n",
      "Epoch: 249/300 - Total Loss: 5.338403273032883\n",
      "Epoch: 250/300 - Total Loss: 5.338402770108961\n",
      "Epoch: 251/300 - Total Loss: 5.338402273265975\n",
      "Epoch: 252/300 - Total Loss: 5.338401782406409\n",
      "Epoch: 253/300 - Total Loss: 5.338401297434697\n",
      "Epoch: 254/300 - Total Loss: 5.338400818257165\n",
      "Epoch: 255/300 - Total Loss: 5.338400344781998\n",
      "Epoch: 256/300 - Total Loss: 5.338399876919186\n",
      "Epoch: 257/300 - Total Loss: 5.33839941458049\n",
      "Epoch: 258/300 - Total Loss: 5.338398957679396\n",
      "Epoch: 259/300 - Total Loss: 5.338398506131071\n",
      "Epoch: 260/300 - Total Loss: 5.338398059852335\n",
      "Epoch: 261/300 - Total Loss: 5.338397618761611\n",
      "Epoch: 262/300 - Total Loss: 5.338397182778896\n",
      "Epoch: 263/300 - Total Loss: 5.338396751825716\n",
      "Epoch: 264/300 - Total Loss: 5.338396325825103\n",
      "Epoch: 265/300 - Total Loss: 5.338395904701553\n",
      "Epoch: 266/300 - Total Loss: 5.33839548838099\n",
      "Epoch: 267/300 - Total Loss: 5.338395076790741\n",
      "Epoch: 268/300 - Total Loss: 5.338394669859504\n",
      "Epoch: 269/300 - Total Loss: 5.338394267517307\n",
      "Epoch: 270/300 - Total Loss: 5.338393869695491\n",
      "Epoch: 271/300 - Total Loss: 5.338393476326671\n",
      "Epoch: 272/300 - Total Loss: 5.338393087344715\n",
      "Epoch: 273/300 - Total Loss: 5.338392702684713\n",
      "Epoch: 274/300 - Total Loss: 5.338392322282948\n",
      "Epoch: 275/300 - Total Loss: 5.33839194607687\n",
      "Epoch: 276/300 - Total Loss: 5.338391574005077\n",
      "Epoch: 277/300 - Total Loss: 5.338391206007281\n",
      "Epoch: 278/300 - Total Loss: 5.3383908420242925\n",
      "Epoch: 279/300 - Total Loss: 5.338390481997988\n",
      "Epoch: 280/300 - Total Loss: 5.33839012587129\n",
      "Epoch: 281/300 - Total Loss: 5.33838977358815\n",
      "Epoch: 282/300 - Total Loss: 5.33838942509352\n",
      "Epoch: 283/300 - Total Loss: 5.338389080333329\n",
      "Epoch: 284/300 - Total Loss: 5.338388739254475\n",
      "Epoch: 285/300 - Total Loss: 5.338388401804788\n",
      "Epoch: 286/300 - Total Loss: 5.338388067933021\n",
      "Epoch: 287/300 - Total Loss: 5.33838773758883\n",
      "Epoch: 288/300 - Total Loss: 5.338387410722746\n",
      "Epoch: 289/300 - Total Loss: 5.3383870872861685\n",
      "Epoch: 290/300 - Total Loss: 5.338386767231341\n",
      "Epoch: 291/300 - Total Loss: 5.3383864505113365\n",
      "Epoch: 292/300 - Total Loss: 5.338386137080033\n",
      "Epoch: 293/300 - Total Loss: 5.338385826892109\n",
      "Epoch: 294/300 - Total Loss: 5.338385519903016\n",
      "Epoch: 295/300 - Total Loss: 5.3383852160689695\n",
      "Epoch: 296/300 - Total Loss: 5.338384915346927\n",
      "Epoch: 297/300 - Total Loss: 5.338384617694582\n",
      "Epoch: 298/300 - Total Loss: 5.338384323070338\n",
      "Epoch: 299/300 - Total Loss: 5.338384031433307\n",
      "Epoch: 300/300 - Total Loss: 5.338383742743286\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2)\n",
    "nn.add_layer(2)\n",
    "nn.add_layer(2)\n",
    "# Each training example has two data points\n",
    "x_train = [\n",
    "    [2.7810836, 2.550537003],\n",
    "    [1.465489372, 2.362125076],\n",
    "    [3.396561688, 4.400293529],\n",
    "    [1.38807019, 1.850220317],\n",
    "    [3.06407232, 3.005305973],\n",
    "    [7.627531214, 2.759262235],\n",
    "    [5.332441248, 2.088626775],\n",
    "    [6.922596716, 1.77106367],\n",
    "    [8.675418651, -0.242068655],\n",
    "    [7.673756466, 3.508563011],\n",
    "]\n",
    "\n",
    "# Each of these outputs is meant to represent a binary value where [1, 0] represents\n",
    "# 0 and [0, 1] represents 1.\n",
    "y_train = [\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "]\n",
    "nn.train(x_train, y_train, 0.5, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39910447523426934, 0.5959140330225479]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict([1.465489372, 2.362125076])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weights': [0.49035573913298885,\n",
       "   -0.004987508798293939,\n",
       "   -1.5029015648555821,\n",
       "   0.041947432991130164,\n",
       "   0.552718553080951,\n",
       "   -1.7496944053192938,\n",
       "   0.2403591600843343,\n",
       "   0.6957052376906776,\n",
       "   0.30516680172159394,\n",
       "   -0.03149015829930516,\n",
       "   0.4539154641670085,\n",
       "   0.08959434081295158,\n",
       "   0.5779310491631121,\n",
       "   0.7349539401696751,\n",
       "   0.35535598344780756,\n",
       "   0.6422487774247264],\n",
       "  'bias': -1.4894766759717697,\n",
       "  'act_val': 0.00864516242132627,\n",
       "  'error': -0.0007810426288416373},\n",
       " {'weights': [0.18158509852918764,\n",
       "   0.38373323540452814,\n",
       "   -1.561847165144747,\n",
       "   0.8258185259593294,\n",
       "   0.8081840063615494,\n",
       "   -1.587807016032515,\n",
       "   0.3667154797519866,\n",
       "   0.8421636029043872,\n",
       "   0.6645892298325686,\n",
       "   0.5276884415923672,\n",
       "   0.4734148106073858,\n",
       "   0.7520614081511627,\n",
       "   0.22301140989013618,\n",
       "   0.03264635952425238,\n",
       "   0.3479214458626019,\n",
       "   0.28018622608981036],\n",
       "  'bias': -1.6157545922067094,\n",
       "  'act_val': 0.008447445120864755,\n",
       "  'error': -0.0023111817423642667}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.desc()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45095581 0.54904419]\n"
     ]
    }
   ],
   "source": [
    "nn.desc()[2][1][\"act_val\"]\n",
    "\n",
    "print(nn.softmax([nn.desc()[2][0][\"act_val\"], nn.desc()[2][1][\"act_val\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
